{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "861622a2",
   "metadata": {},
   "source": [
    "Step 1: select small dataset from largedataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45a2542",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your original dataset\n",
    "df = pd.read_csv(\"financial_risk_data.csv\")\n",
    "\n",
    "# Take only first 500 rows\n",
    "df_small = df.head(500)\n",
    "\n",
    "# Save it as a new file\n",
    "df_small.to_csv(\"small_dataset.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70742ce",
   "metadata": {},
   "source": [
    "Step 2: show overview of small dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241eca55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv(\"small_dataset.csv\")\n",
    "\n",
    "# Basic shape of the data\n",
    "print(\"üßæ Dataset Shape:\", df.shape)\n",
    "\n",
    "# First 5 rows\n",
    "print(\"\\nüîç First 5 Rows:\")\n",
    "display(df.head())\n",
    "\n",
    "# Info about columns and data types\n",
    "print(\"\\n‚ÑπÔ∏è Dataset Info:\")\n",
    "df.info()\n",
    "\n",
    "# Summary statistics for numeric columns\n",
    "print(\"\\nüìä Summary Statistics:\")\n",
    "display(df.describe())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\n‚ùó Missing Values in Each Column:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Check for duplicate rows\n",
    "print(\"\\nüìã Number of Duplicate Rows:\", df.duplicated().sum())\n",
    "\n",
    "# Display column names\n",
    "print(\"\\nü™∂ Column Names:\")\n",
    "print(df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224ce33f",
   "metadata": {},
   "source": [
    "Step 2: Data cleaning and fill data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06690e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- STEP 3: CLEANING & PREPROCESSING (paste into your notebook) ----------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# ---------- CONFIG ----------\n",
    "TARGET_COL = 'loan_status'   # change if different\n",
    "ID_COLS = ['id', 'member_id', 'url', 'title', 'zip_code', 'desc']  # drop these if present\n",
    "MAX_MISSING_RATIO = 0.80     # drop columns with >80% missing\n",
    "CAT_CARDINALITY_LIMIT = 15   # only one-hot encode categoricals with unique <= this\n",
    "TEST_SIZE_FRACTION = 0.20    # fallback fraction for larger datasets\n",
    "RANDOM_STATE = 42\n",
    "# ----------------------------\n",
    "\n",
    "# Copy df to avoid accidental mutation\n",
    "df_clean = df.copy()\n",
    "\n",
    "print(\"Initial shape:\", df_clean.shape)\n",
    "\n",
    "# 1) Drop columns with too many missing values\n",
    "missing_ratio = df_clean.isna().mean()\n",
    "cols_to_drop = missing_ratio[missing_ratio > MAX_MISSING_RATIO].index.tolist()\n",
    "print(f\"Dropping {len(cols_to_drop)} columns with >{int(MAX_MISSING_RATIO*100)}% missing.\")\n",
    "df_clean.drop(columns=cols_to_drop, inplace=True, errors='ignore')\n",
    "\n",
    "# 2) Drop obvious identifier columns if present\n",
    "existing_ids = [c for c in ID_COLS if c in df_clean.columns]\n",
    "if existing_ids:\n",
    "    print(\"Dropping identifier cols:\", existing_ids)\n",
    "    df_clean.drop(columns=existing_ids, inplace=True, errors='ignore')\n",
    "\n",
    "print(\"Shape after column drops:\", df_clean.shape)\n",
    "\n",
    "# 3) Normalize / prepare some known columns\n",
    "# 3a) term -> months (e.g. '36 months' -> 36)\n",
    "if 'term' in df_clean.columns:\n",
    "    df_clean['term_months'] = df_clean['term'].astype(str).str.extract(r'(\\d+)').astype(float)\n",
    "    df_clean.drop(columns=['term'], inplace=True, errors='ignore')\n",
    "\n",
    "# 3b) percent-like columns that might be strings with '%' (int_rate, revol_util, sec_app_revol_util, etc.)\n",
    "percent_cols = [c for c in df_clean.columns if df_clean[c].dtype == 'object' and df_clean[c].astype(str).str.contains('%').any()]\n",
    "# also include common percent-named columns if present\n",
    "for c in ['int_rate', 'revol_util', 'sec_app_revol_util']:\n",
    "    if c in df_clean.columns and df_clean[c].dtype == object:\n",
    "        percent_cols.append(c)\n",
    "percent_cols = list(set(percent_cols))\n",
    "for c in percent_cols:\n",
    "    try:\n",
    "        df_clean[c] = df_clean[c].astype(str).str.replace('%', '').str.strip()\n",
    "        df_clean[c] = pd.to_numeric(df_clean[c], errors='coerce')\n",
    "        print(f\"Converted percent-col -> numeric: {c}\")\n",
    "    except Exception as e:\n",
    "        print(\"Could not convert percent col:\", c, \"->\", e)\n",
    "\n",
    "# 3c) If int_rate already numeric but in form '13.99' it's fine. If still object, try to coerce.\n",
    "if 'int_rate' in df_clean.columns and not np.issubdtype(df_clean['int_rate'].dtype, np.number):\n",
    "    df_clean['int_rate'] = pd.to_numeric(df_clean['int_rate'], errors='coerce')\n",
    "\n",
    "# 4) Prepare target (binary)\n",
    "if TARGET_COL not in df_clean.columns:\n",
    "    raise ValueError(f\"Target column '{TARGET_COL}' not found in dataframe.\")\n",
    "\n",
    "# If already numeric and 0/1 -> use directly; otherwise map common labels\n",
    "if pd.api.types.is_numeric_dtype(df_clean[TARGET_COL].dtype):\n",
    "    unique_vals = np.unique(df_clean[TARGET_COL].dropna().astype(int))\n",
    "    if set(unique_vals).issubset({0, 1}):\n",
    "        df_clean['target'] = df_clean[TARGET_COL].astype(int)\n",
    "        print(\"Target already binary numeric (0/1).\")\n",
    "    else:\n",
    "        # fallback: treat non-zero as 1\n",
    "        df_clean['target'] = (df_clean[TARGET_COL] != 0).astype(int)\n",
    "        print(\"Target numeric but not 0/1; mapped non-zero -> 1.\")\n",
    "else:\n",
    "    # common label mapping (adjust if your dataset uses different labels)\n",
    "    label_map = {\n",
    "        'Fully Paid': 0,\n",
    "        'Current': 0,\n",
    "        'In Grace Period': 0,\n",
    "        'Late (16-30 days)': 1,\n",
    "        'Late (31-120 days)': 1,\n",
    "        'Charged Off': 1,\n",
    "        'Default': 1,\n",
    "        'charged off': 1,\n",
    "        'Fully Paid.': 0\n",
    "    }\n",
    "    unknown_labels = set(df_clean[TARGET_COL].dropna().unique()) - set(label_map.keys())\n",
    "    if unknown_labels:\n",
    "        print(\"Warning: Unknown target labels found (will be dropped):\", unknown_labels)\n",
    "    df_clean['target'] = df_clean[TARGET_COL].map(label_map)\n",
    "    # drop rows where mapping failed\n",
    "    before_rows = len(df_clean)\n",
    "    df_clean = df_clean[df_clean['target'].notna()].copy()\n",
    "    print(f\"Dropped {before_rows - len(df_clean)} rows with unmapped target labels.\")\n",
    "\n",
    "# convert to int\n",
    "df_clean['target'] = df_clean['target'].astype(int)\n",
    "print(\"Target distribution:\\n\", df_clean['target'].value_counts())\n",
    "\n",
    "# 5) Feature selection: numeric features + select low-cardinality categorical features\n",
    "numeric_cols = df_clean.select_dtypes(include=[np.number]).columns.tolist()\n",
    "# remove target from numeric features\n",
    "numeric_cols = [c for c in numeric_cols if c not in ['target']]\n",
    "\n",
    "# candidate categorical columns (object / category types)\n",
    "cat_cols = df_clean.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "# keep only low-cardinality categorical columns for one-hot\n",
    "cat_keep = []\n",
    "for c in cat_cols:\n",
    "    n_unique = df_clean[c].nunique(dropna=True)\n",
    "    if 1 < n_unique <= CAT_CARDINALITY_LIMIT:\n",
    "        cat_keep.append(c)\n",
    "\n",
    "print(f\"Selected {len(numeric_cols)} numeric features and {len(cat_keep)} categorical features for encoding.\")\n",
    "print(\"Numeric (sample):\", numeric_cols[:10])\n",
    "print(\"Categorical to encode (sample):\", cat_keep[:10])\n",
    "\n",
    "# Build X\n",
    "X_num = df_clean[numeric_cols].copy()\n",
    "X_cat = df_clean[cat_keep].copy() if cat_keep else pd.DataFrame(index=df_clean.index)\n",
    "\n",
    "# 6) Impute numeric with median\n",
    "num_imputer = SimpleImputer(strategy='median')\n",
    "if not X_num.empty:\n",
    "    X_num_imputed = pd.DataFrame(num_imputer.fit_transform(X_num), columns=X_num.columns, index=X_num.index)\n",
    "else:\n",
    "    X_num_imputed = X_num\n",
    "\n",
    "# 7) Fill categorical missing with 'MISSING' and one-hot encode\n",
    "if not X_cat.empty:\n",
    "    X_cat_filled = X_cat.fillna('MISSING').astype(str)\n",
    "    X_cat_encoded = pd.get_dummies(X_cat_filled, drop_first=True)\n",
    "else:\n",
    "    X_cat_encoded = pd.DataFrame(index=X_num_imputed.index)\n",
    "\n",
    "# 8) Combine numeric + categorical\n",
    "X = pd.concat([X_num_imputed, X_cat_encoded], axis=1)\n",
    "print(\"Final feature matrix shape (before scaling):\", X.shape)\n",
    "\n",
    "# 9) Safety check - if X has any NaN (shouldn't) replace with median fallback\n",
    "if X.isna().any().any():\n",
    "    print(\"Warning: NaNs found in X after imputation. Applying median fallback for remaining NaNs.\")\n",
    "    X = pd.DataFrame(num_imputer.fit_transform(X), columns=X.columns, index=X.index)\n",
    "\n",
    "# 10) Optional scaling (helpful for many models)\n",
    "scaler = StandardScaler()\n",
    "if X.shape[1] > 0:\n",
    "    X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns, index=X.index)\n",
    "else:\n",
    "    X_scaled = X\n",
    "\n",
    "# 11) Prepare y and safe train-test split (absolute test count)\n",
    "y = df_clean['target'].reset_index(drop=True)\n",
    "X_scaled = X_scaled.reset_index(drop=True)\n",
    "\n",
    "n_samples = len(X_scaled)\n",
    "if n_samples == 0:\n",
    "    raise ValueError(\"No samples available after preprocessing.\")\n",
    "\n",
    "n_test = max(1, int(round(TEST_SIZE_FRACTION * n_samples)))\n",
    "if n_test >= n_samples:\n",
    "    n_test = n_samples - 1\n",
    "print(f\"Using absolute test size = {n_test} of {n_samples} samples.\")\n",
    "\n",
    "# stratify if each class has >=2 samples\n",
    "stratify_arg = None\n",
    "vc = y.value_counts()\n",
    "if vc.min() >= 2:\n",
    "    stratify_arg = y\n",
    "    print(\"Using stratified split.\")\n",
    "else:\n",
    "    print(\"Not enough samples per class to stratify; doing random split.\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=n_test, random_state=RANDOM_STATE, stratify=stratify_arg\n",
    ")\n",
    "\n",
    "print(\"Train shape:\", X_train.shape, \"Test shape:\", X_test.shape)\n",
    "print(\"Train target distribution:\\n\", y_train.value_counts())\n",
    "print(\"Test target distribution:\\n\", y_test.value_counts())\n",
    "\n",
    "# 12) Save processed datasets into workspace variables for next step\n",
    "processed = {\n",
    "    'X_train': X_train, 'X_test': X_test,\n",
    "    'y_train': y_train, 'y_test': y_test,\n",
    "    'X_all': X_scaled, 'y_all': y\n",
    "}\n",
    "print(\"Preprocessing complete. Processed dict created for model training (use processed['X_train'] etc.).\")\n",
    "# ----------------------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cad56a0",
   "metadata": {},
   "source": [
    "Step 4: Model Training & Evaluation (Logistic Regression)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5375add2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß† Step 4: Model Training & Evaluation (Logistic Regression)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# üß© Extract processed data from your step 3 dictionary\n",
    "X_train = processed[\"X_train\"]\n",
    "X_test = processed[\"X_test\"]\n",
    "y_train = processed[\"y_train\"]\n",
    "y_test = processed[\"y_test\"]\n",
    "\n",
    "print(f\"Training shape: {X_train.shape}, Testing shape: {X_test.shape}\")\n",
    "\n",
    "# ‚öôÔ∏è 1) Initialize Logistic Regression model\n",
    "model = LogisticRegression(max_iter=2000, solver='lbfgs', class_weight='balanced', random_state=42)\n",
    "\n",
    "# üöÄ 2) Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# üîç 3) Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# üìä 4) Evaluate performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"\\n‚úÖ Model Evaluation Results:\")\n",
    "print(f\"Accuracy: {accuracy:.3f}\\n\")\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# üß© 5) Confusion Matrix Visualization\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Blues')\n",
    "plt.title(\"Confusion Matrix - Logistic Regression\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.show()\n",
    "\n",
    "# üíæ 6) Save model for later use\n",
    "import pickle\n",
    "pickle.dump(model, open(\"model.pkl\", \"wb\"))\n",
    "print(\"Model saved as model.pkl ‚úÖ\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
